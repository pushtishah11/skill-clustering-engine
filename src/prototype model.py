# -*- coding: utf-8 -*-
"""proto.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yJky13wqkhScl5wOAzaGNrXfKCrrPyay
"""

!pip install sentence_transformers
from sentence_transformers import SentenceTransformer
import numpy as np, pandas as pd, json, re, os

!pip install HDBSCAN

from collections import defaultdict
from sklearn.cluster import HDBSCAN

#MODEL_NAME = "all-MiniLM-L6-v2"
MODEL_NAME = "multi-qa-mpnet-base-cos-v1"
SEED_CSV = "/content/Mega_Skills_List__full_.csv"      # column: skill
INCOMING_JSON = "/content/skills(new).json"     # ES agg format
TAU = 0.58   # reject threshold (cosine)
#DELTA = 0.04  # top-2 gap for AMBIGUOUS

KEEP_BIGRAMS = {
    "aws s3","aws ec2","amazon rds","amazon dynamodb","sql server",
    "google cloud","google bigquery","azure ad","azure devops",
    "spring boot","entity framework","react native","rest api","web socket"
}

def normalize(txt: str) -> str:
    s = str(txt).lower().strip()
    s = re.sub(r"[_/|]+"," ", s)
    s = re.sub(r"[^\w\s\+\.\#-]", " ", s)     # keep + . # - (C++/.NET)
    s = re.sub(r"\s+", " ", s).strip()
    s = re.sub(r"\b(developer|engineer|framework|core|architect|junior|senior|lead|intern|beginner|programming|basics|basic|advanced|knowledge of|advance|of|technology|technologies)", "", s)

    for bg in KEEP_BIGRAMS:
        s = s.replace(bg, bg.replace(" ","_")) # protect bigrams
    return s
''' Normalizes + cleans up the tezt '''

def l2(x: np.ndarray):
    x = np.asarray(x, dtype=np.float32)
    n = np.linalg.norm(x, axis=1, keepdims=True) + 1e-12
    return x / n

'''Converts x to a float32 NumPy array.
- Computes the L2 norm (length) of each row.
- Divides each row by its length, so every row has length ~1.
- Adding 1e-12 prevents division by zero.
- Why? With unit-length vectors, the dot product Q @ V.T is exactly cosine similarity. That’s fast and stable.
'''

def build_prototypes(seed_csv=SEED_CSV, model_name=MODEL_NAME):
    df = pd.read_csv(seed_csv)
    names = (
        df["skill"].dropna().astype(str).str.strip()
        .replace("", np.nan).dropna().unique().tolist()
    )
    norms = [normalize(s) for s in names]
    model = SentenceTransformer(model_name)
    V = model.encode(norms, convert_to_numpy=True)
    V = l2(V)  # spherical/cosine space

    np.save("proto_vectors.npy", V)
    with open("proto_names.json","w",encoding="utf-8") as f:
        json.dump(names, f, ensure_ascii=False)
    print(f"Saved {len(names)} prototypes.")

''' prototype for each skill from the list is made '''

def sort_for_csv(df: pd.DataFrame):
    df = df.copy()
    # put REJECT first, then OK; then sort by match name and confidence desc
    df["status"] = pd.Categorical(df["status"], categories=["REJECT","OK"], ordered=True)
    return df.sort_values(["status", "match", "confidence"], ascending=[True, True, False])
''' function for the csv output file'''

def promote_reject_clusters_to_prototypes(
    clusters_dict: dict,                     # kept for backward-compat; not required now
    rej_df_with_labels: pd.DataFrame,        # must include rc_label, rc_prob, rc_rep, input
    seed_csv: str = SEED_CSV,
    min_size: int = 8,                       # promote clusters with ≥ this many items
    min_avg_prob: float = 0.55,              # and avg membership prob ≥ this
):
    """
    Promote strong reject-clusters using their representative 'rc_rep'
    (medoid or count-max keyword, depending on how cluster_rejects was called).

    Returns: list[str] of promoted canonical skill names.
    """
    # --- sanity checks
    needed = {"rc_label", "rc_prob", "rc_rep", "input"}
    if not needed.issubset(rej_df_with_labels.columns):
        raise ValueError(f"rej_df_with_labels must include columns: {needed}")

    # load existing seeds
    seeds = pd.read_csv(seed_csv)
    if "skill" not in seeds.columns:
        raise ValueError(f"{seed_csv} must contain a 'skill' column")
    seen = set(x.strip().lower() for x in seeds["skill"].astype(str))

    # compute stats per cluster (ignore noise = -1)
    stats = (
        rej_df_with_labels[rej_df_with_labels["rc_label"] != -1]
        .groupby("rc_label")
        .agg(size=("input", "count"), avg_prob=("rc_prob", "mean"))
        .reset_index()
    )

    eligible = stats[(stats["size"] >= min_size) & (stats["avg_prob"] >= min_avg_prob)]
    if eligible.empty:
        print("No reject-clusters met the promotion thresholds.")
        return []

    # map cluster -> representative (take any non-empty rc_rep for that cluster)
    rep_per_cluster = (
        rej_df_with_labels[rej_df_with_labels["rc_label"] != -1]
        .loc[:, ["rc_label", "rc_rep"]]
        .drop_duplicates()
        .groupby("rc_label")["rc_rep"]
        .first()  # rc_rep is constant per cluster; first is fine
        .to_dict()
    )

    to_add = []
    for lab in eligible["rc_label"].astype(int):
        rep = rep_per_cluster.get(lab, "")
        if not rep:  # fallback: if rc_rep somehow empty, use the most frequent input in that cluster
            items = (
                rej_df_with_labels[rej_df_with_labels["rc_label"] == lab]["input"]
                .astype(str)
                .tolist()
            )
            if not items:
                continue
            rep = items[0]

        # normalize to canonical form; restore underscores to spaces
        canon = normalize(rep).replace("_", " ").strip()
        if canon and canon.lower() not in seen:
            to_add.append(canon)

    if not to_add:
        print("Eligible clusters found, but all representatives already in seed list.")
        return []

    # append & dedupe
    new_rows = pd.DataFrame({"skill": to_add})
    seeds2 = (
        pd.concat([seeds, new_rows], ignore_index=True)
        .dropna()
        .drop_duplicates(subset=["skill"], keep="first")
        .reset_index(drop=True)
    )
    seeds2.to_csv(seed_csv, index=False)
    print(f"Promoted {len(to_add)} new prototypes → appended to {seed_csv}")
    return to_add

# stoplist for count_max
_STOP = {
    "the","a","an","and","or","of","for","to","in","on","with","by","at",
    "dev","developer","engineer","senior","junior","lead","intern","basic",
    "basics","advanced","advance","programming","technology","technologies"
}

def _tokens_for_count_max(text: str) -> list[str]:
    s = str(text).lower().strip()
    toks = re.split(r"[ _]+", s)
    out = []
    for t in toks:
        t = t.strip()
        if not t or t in _STOP or len(t) < 2 or t.isdigit():
            continue
        out.append(t)
    return out

def cluster_rejects(
    df_predictions: pd.DataFrame,
    model_name: str = MODEL_NAME,
    min_cluster_size: int = 2,
    min_samples: int = 1,
    cluster_selection_method: str = "leaf",
    cluster_selection_epsilon: float = 0.55,
    representative: str = "medoid",   # "medoid" or "count_max"
):
    """
    Cluster only the REJECT rows using HDBSCAN and attach labels/probabilities + a representative string.
    Returns: (preds_with_rc_cols, clusters_dict)
      - preds_with_rc_cols: original DF + ['rc_label','rc_prob','rc_rep']
      - clusters_dict: {label: [list of input strings]}
    Writes: reject_clusters.csv, reject_clusters.json, reject_cluster_labels.json
    """
    # 1) Filter rejects
    rej = df_predictions[df_predictions["status"] == "REJECT"].copy()
    if rej.empty:
        print("No rejects to cluster.")
        out = df_predictions.assign(
            rc_label=pd.Series(dtype=int),
            rc_prob=pd.Series(dtype=float),
            rc_rep=pd.Series(dtype=object),
        )
        return out, {}

    # 2) Embed rejects (unit vectors)
    model = SentenceTransformer(model_name)
    texts_norm = rej["norm"].astype(str).tolist()
    texts_raw  = rej["input"].astype(str).tolist()
    X = model.encode(texts_norm, convert_to_numpy=True, normalize_embeddings=True)

    # 3) HDBSCAN fit — ask it to store medoids if available
    #    (older versions ignore 'store_centers' -> we handle fallback below)
    try:
        clusterer = hdbscan.HDBSCAN(
            min_cluster_size=min_cluster_size,
            min_samples=min_samples,
            metric="euclidean",
            cluster_selection_method=cluster_selection_method,
            cluster_selection_epsilon=cluster_selection_epsilon,
            store_centers="medoid",
        ).fit(X)
    except TypeError:
        # older hdbscan without 'store_centers'
        clusterer = hdbscan.HDBSCAN(
            min_cluster_size=min_cluster_size,
            min_samples=min_samples,
            metric="euclidean",
            cluster_selection_method=cluster_selection_method,
            cluster_selection_epsilon=cluster_selection_epsilon,
        ).fit(X)

    labels = clusterer.labels_          # -1 = noise
    probs  = clusterer.probabilities_

    # 4) Attach back to rejects
    rej["rc_label"] = labels
    rej["rc_prob"]  = probs

    # 5) Build {cluster_id: [items]} and index lists
    clusters_dict = defaultdict(list)
    label_to_indices = defaultdict(list)
    for i, lab in enumerate(labels):
        if lab != -1:
            clusters_dict[int(lab)].append(texts_raw[i])
            label_to_indices[int(lab)].append(i)
    clusters_dict = dict(clusters_dict)

    # 6) Representatives
    rep_map: dict[int, str] = {}
    if representative not in {"medoid", "count_max"}:
        raise ValueError("representative must be 'medoid' or 'count_max'")

    if representative == "medoid":
        # Prefer built-in medoids_ if present
        if hasattr(clusterer, "medoids_") and clusterer.medoids_ is not None:
            # hdbscan labels clusters 0..K-1; medoids_ gives global indices into X
            for lab, idxs in label_to_indices.items():
                try:
                    medoid_global_idx = int(clusterer.medoids_[lab])
                    rep_map[lab] = texts_raw[medoid_global_idx]
                except Exception:
                    # Fallback: manual medoid within this cluster
                    Xk = X[idxs]
                    G = Xk @ Xk.T
                    D = np.sqrt(np.maximum(0.0, 2.0 - 2.0 * G))
                    medoid_local = int(np.argmin(D.sum(axis=1)))
                    rep_map[lab] = texts_raw[idxs[medoid_local]]
        else:
            # Manual medoid (works on any version)
            for lab, idxs in label_to_indices.items():
                Xk = X[idxs]
                G = Xk @ Xk.T
                D = np.sqrt(np.maximum(0.0, 2.0 - 2.0 * G))
                medoid_local = int(np.argmin(D.sum(axis=1)))
                rep_map[lab] = texts_raw[idxs[medoid_local]]

    else:  # representative == "count_max"
        for lab, idxs in label_to_indices.items():
            bag = Counter()
            for i in idxs:
                bag.update(_tokens_for_count_max(texts_norm[i]))
            if bag:
                maxc = max(bag.values())
                cands = [t for t, c in bag.items() if c == maxc]
                cands.sort(key=lambda t: (-len(t), t))  # prefer longer token
                rep_map[lab] = cands[0]
            else:
                rep_map[lab] = texts_raw[idxs[0]]

    # 7) Save + merge
    rej["rc_rep"] = rej["rc_label"].apply(lambda k: rep_map.get(int(k), ""))

    rej_sorted = rej.sort_values(["rc_label","rc_prob","input"],
                                 ascending=[True, False, True])
    rej_sorted.to_csv("reject_clusters.csv", index=False)

    with open("reject_clusters.json","w",encoding="utf-8") as f:
        json.dump(clusters_dict, f, ensure_ascii=False, indent=2)
    with open("reject_cluster_labels.json","w",encoding="utf-8") as f:
        json.dump({int(k): v for k, v in rep_map.items()}, f, ensure_ascii=False, indent=2)

    print(
        f"Saved: reject_clusters.csv, reject_clusters.json, reject_cluster_labels.json "
        f"(clusters={len(clusters_dict)}; noise={(labels==-1).sum()}; rep='{representative}')"
    )

    out = df_predictions.copy()
    out = out.merge(rej_sorted[["input","rc_label","rc_prob","rc_rep"]],
                    on="input", how="left")
    return out, clusters_dict

def promote_reject_clusters_to_prototypes(
    clusters_dict: dict,
    rej_df_with_labels: pd.DataFrame,
    seed_csv: str = SEED_CSV,
    min_size: int = 8,            # promote clusters with ≥ this many items
    min_avg_prob: float = 0.55,   # and average HDBSCAN membership ≥ this
):
    """
    Works with clusters_dict = {cluster_label: [items]} (no medoid stored).
    Picks the first item in each cluster as the exemplar.
    Appends promoted exemplars to seed_csv (expects a 'skill' column).
    Returns: list of promoted skill strings.
    """
    if not clusters_dict:
        print("No clusters to consider for promotion.")
        return []

    # Load existing seeds
    seeds = pd.read_csv(seed_csv)
    if "skill" not in seeds.columns:
        raise ValueError(f"{seed_csv} must contain a 'skill' column")
    seen = set(x.strip().lower() for x in seeds["skill"].astype(str).tolist())

    # Sanity checks on rejects view
    req_cols = {"rc_label", "rc_prob", "input"}
    if not req_cols.issubset(rej_df_with_labels.columns):
        raise ValueError(f"rej_df_with_labels must include {req_cols}")


    # Compute cluster stats from rejects (ignore noise = -1)
    stats = (
        rej_df_with_labels[rej_df_with_labels["rc_label"] != -1]
        .groupby("rc_label")
        .agg(size=("input", "count"), avg_prob=("rc_prob", "mean"))
        .reset_index()
    )

    # Determine eligible clusters
    eligible = stats[(stats["size"] >= min_size) & (stats["avg_prob"] >= min_avg_prob)]
    if eligible.empty:
        print("No reject-clusters met the promotion thresholds.")
        return []

    to_add = []
    for _, row in eligible.iterrows():
        lab = int(row["rc_label"])
        items = clusters_dict.get(lab, [])
        if not items:
            continue
        exemplar = items[0]  # first item as exemplar
        canon = normalize(exemplar).replace("_", " ").strip()
        if canon and canon.lower() not in seen:
            to_add.append(canon)

    if not to_add:
        print("Eligible clusters found, but all exemplars already in seed list.")
        return []

    # Append & dedupe
    new_rows = pd.DataFrame({"skill": to_add})
    seeds2 = (
        pd.concat([seeds, new_rows], ignore_index=True)
        .dropna()
        .drop_duplicates(subset=["skill"], keep="first")
        .reset_index(drop=True)
    )
    seeds2.to_csv(seed_csv, index=False)
    print(f"Promoted {len(to_add)} new prototypes → appended to {seed_csv}")
    return to_add

#count max count,
#if set to medoid, use that

if __name__ == "__main__":

    build_prototypes()
    preds = predict_incoming()

    # Cluster the rejects
    preds2, clusters = cluster_rejects(
    preds,
    model_name=MODEL_NAME,
    min_cluster_size=2,
    min_samples=1,
    cluster_selection_method="leaf",
    cluster_selection_epsilon=0.55,
    representative="medoid",     # or "count_max" depending on what is wanted
)

    preds2.to_csv("proto_predictions_with_reject_clusters.csv", index=False)

    #  Auto-promote strong reject-clusters into the seed list
    rejects_view = preds2.loc[preds2["status"] == "REJECT", ["input", "rc_label", "rc_prob"]].copy()
    promoted = promote_reject_clusters_to_prototypes(
        clusters_dict=rc,
        rej_df_with_labels=rejects_view,
        seed_csv=SEED_CSV,
        min_size=5,
        min_avg_prob=0.55,
    )

    #  If we promoted anything, rebuild prototypes and re-run predictions (optional)
    if promoted:
        print(f"Rebuilding prototypes with {len(promoted)} new skills…")
        build_prototypes(seed_csv=SEED_CSV)
        preds_updated = predict_incoming(incoming_json=INCOMING_JSON, tau=TAU)
        preds_updated.to_csv("proto_predictions_after_promotion.csv", index=False)
        print("Saved: proto_predictions_after_promotion.csv")

